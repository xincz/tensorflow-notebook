{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\n",
      "matplotlib 3.3.2\n",
      "numpy 1.18.4\n",
      "pandas 1.1.4\n",
      "sklearn 0.23.2\n",
      "tensorflow 2.3.1\n",
      "tensorflow.keras 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: \"machine learning\"\n",
      "value: \"cc150\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tfrecord 文件格式\n",
    "# -> tf.train.Example\n",
    "#   -> tf.train.Features -> {\"key\": tf.train.Features}\n",
    "#     -> tf.train.Feature.ByteList/FloatList/Int64List\n",
    "\n",
    "favorite_books = [name.encode(\"utf-8\") for name in [\"machine learning\", \"cc150\"]]\n",
    "favorite_books_bytelist = tf.train.BytesList(value=favorite_books)\n",
    "print(favorite_books_bytelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 15.5\n",
      "value: 9.5\n",
      "value: 5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hours_floatlist = tf.train.FloatList(value=[15.5, 9.5, 5.])\n",
    "print(hours_floatlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 15\n",
      "value: 9\n",
      "value: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "age_int64list = tf.train.Int64List(value=[15, 9, 5])\n",
    "print(age_int64list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  key: \"age\"\n",
      "  value {\n",
      "    int64_list {\n",
      "      value: 15\n",
      "      value: 9\n",
      "      value: 5\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  key: \"favorite_books\"\n",
      "  value {\n",
      "    bytes_list {\n",
      "      value: \"machine learning\"\n",
      "      value: \"cc150\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  key: \"hours\"\n",
      "  value {\n",
      "    float_list {\n",
      "      value: 15.5\n",
      "      value: 9.5\n",
      "      value: 5.0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = tf.train.Features(\n",
    "feature = {\n",
    "    \"favorite_books\": tf.train.Feature(bytes_list=favorite_books_bytelist),\n",
    "    \"hours\": tf.train.Feature(float_list=hours_floatlist),\n",
    "    \"age\": tf.train.Feature(int64_list=age_int64list)\n",
    "})\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"age\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 15\n",
      "        value: 9\n",
      "        value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"favorite_books\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"machine learning\"\n",
      "        value: \"cc150\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"hours\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 15.5\n",
      "        value: 9.5\n",
      "        value: 5.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = tf.train.Example(features=features)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\nZ\\n\\x0e\\n\\x03age\\x12\\x07\\x1a\\x05\\n\\x03\\x0f\\t\\x05\\n\\x19\\n\\x05hours\\x12\\x10\\x12\\x0e\\n\\x0c\\x00\\x00xA\\x00\\x00\\x18A\\x00\\x00\\xa0@\\n-\\n\\x0efavorite_books\\x12\\x1b\\n\\x19\\n\\x10machine learning\\n\\x05cc150'\n"
     ]
    }
   ],
   "source": [
    "serialized_example = example.SerializePartialToString()\n",
    "print(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"tfrecord_basic\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "filename = \"test.tfrecords\"\n",
    "filename_fullpath = os.path.join(output_dir, filename)\n",
    "with tf.io.TFRecordWriter(filename_fullpath) as writer:\n",
    "    for i in range(3):\n",
    "        writer.write(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\nZ\\n\\x0e\\n\\x03age\\x12\\x07\\x1a\\x05\\n\\x03\\x0f\\t\\x05\\n\\x19\\n\\x05hours\\x12\\x10\\x12\\x0e\\n\\x0c\\x00\\x00xA\\x00\\x00\\x18A\\x00\\x00\\xa0@\\n-\\n\\x0efavorite_books\\x12\\x1b\\n\\x19\\n\\x10machine learning\\n\\x05cc150', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\nZ\\n\\x0e\\n\\x03age\\x12\\x07\\x1a\\x05\\n\\x03\\x0f\\t\\x05\\n\\x19\\n\\x05hours\\x12\\x10\\x12\\x0e\\n\\x0c\\x00\\x00xA\\x00\\x00\\x18A\\x00\\x00\\xa0@\\n-\\n\\x0efavorite_books\\x12\\x1b\\n\\x19\\n\\x10machine learning\\n\\x05cc150', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\nZ\\n\\x0e\\n\\x03age\\x12\\x07\\x1a\\x05\\n\\x03\\x0f\\t\\x05\\n\\x19\\n\\x05hours\\x12\\x10\\x12\\x0e\\n\\x0c\\x00\\x00xA\\x00\\x00\\x18A\\x00\\x00\\xa0@\\n-\\n\\x0efavorite_books\\x12\\x1b\\n\\x19\\n\\x10machine learning\\n\\x05cc150', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "dataset = tf.data.TFRecordDataset([filename_fullpath])\n",
    "for serialized_example_tensor in dataset:\n",
    "    print(serialized_example_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Feature: age_int64list (data type: int64) is required but could not be found. [Op:ParseExampleV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-45ec6170178c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     example = tf.io.parse_single_example(\n\u001b[0;32m     10\u001b[0m         \u001b[0mserialized_example_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         expected_features)\n\u001b[0m\u001b[0;32m     12\u001b[0m     books = tf.sparse.to_dense(example[\"favorite_books\"],\n\u001b[0;32m     13\u001b[0m                                default_value=b\"\")\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py\u001b[0m in \u001b[0;36mparse_single_example_v2\u001b[1;34m(serialized, features, example_names, name)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[0mserialized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"serialized\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[0mserialized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_assert_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"serialized\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparse_example_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py\u001b[0m in \u001b[0;36mparse_example_v2\u001b[1;34m(serialized, features, example_names, name)\u001b[0m\n\u001b[0;32m    312\u001b[0m   ])\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_example_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0m_construct_tensors_for_composite_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py\u001b[0m in \u001b[0;36m_parse_example_raw\u001b[1;34m(serialized, names, params, name)\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[0mragged_split_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mragged_split_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mdense_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_shapes_as_proto\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    363\u001b[0m     (sparse_indices, sparse_values, sparse_shapes, dense_values,\n\u001b[0;32m    364\u001b[0m      ragged_values, ragged_row_splits) = outputs\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_parsing_ops.py\u001b[0m in \u001b[0;36mparse_example_v2\u001b[1;34m(serialized, names, sparse_keys, dense_keys, ragged_keys, dense_defaults, num_sparse, sparse_types, ragged_value_types, ragged_split_types, dense_shapes, name)\u001b[0m\n\u001b[0;32m    743\u001b[0m           \u001b[0mragged_value_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mragged_value_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m           \u001b[0mragged_split_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mragged_split_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdense_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m           name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m    746\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m       \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_parsing_ops.py\u001b[0m in \u001b[0;36mparse_example_v2_eager_fallback\u001b[1;34m(serialized, names, sparse_keys, dense_keys, ragged_keys, dense_defaults, num_sparse, sparse_types, ragged_value_types, ragged_split_types, dense_shapes, name, ctx)\u001b[0m\n\u001b[0;32m    836\u001b[0m                              \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mragged_value_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m                              \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mragged_split_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_inputs_flat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[0;32m    839\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m     _execute.record_gradient(\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Feature: age_int64list (data type: int64) is required but could not be found. [Op:ParseExampleV2]"
     ]
    }
   ],
   "source": [
    "# Deserialize\n",
    "expected_features = {\n",
    "    \"favorite_books\": tf.io.VarLenFeature(dtype=tf.string),\n",
    "    \"hours\": tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    \"age\": tf.io.FixedLenFeature([], dtype=tf.int64)\n",
    "}\n",
    "dataset = tf.data.TFRecordDataset([filename_fullpath])\n",
    "for serialized_example_tensor in dataset:\n",
    "    example = tf.io.parse_single_example(\n",
    "        serialized_example_tensor,\n",
    "        expected_features)\n",
    "    books = tf.sparse.to_dense(example[\"favorite_books\"],\n",
    "                               default_value=b\"\")\n",
    "    for book in books:\n",
    "        print(book.numpy().decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_fullpath_zip = filename_fullpath + '.zip'\n",
    "options = tf.io.TFRecordOptions(compression_type = \"GZIP\")\n",
    "with tf.io.TFRecordWriter(filename_fullpath_zip, options) as writer:\n",
    "    for i in range(3):\n",
    "        writer.write(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Feature: age_int64list (data type: int64) is required but could not be found. [Op:ParseExampleV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-61388276372f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     example = tf.io.parse_single_example(\n\u001b[0;32m      5\u001b[0m         \u001b[0mserialized_example_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         expected_features)\n\u001b[0m\u001b[0;32m      7\u001b[0m     books = tf.sparse.to_dense(example[\"favorite_books\"],\n\u001b[0;32m      8\u001b[0m                                default_value=b\"\")\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py\u001b[0m in \u001b[0;36mparse_single_example_v2\u001b[1;34m(serialized, features, example_names, name)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[0mserialized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"serialized\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[0mserialized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_assert_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"serialized\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparse_example_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py\u001b[0m in \u001b[0;36mparse_example_v2\u001b[1;34m(serialized, features, example_names, name)\u001b[0m\n\u001b[0;32m    312\u001b[0m   ])\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_example_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0m_construct_tensors_for_composite_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py\u001b[0m in \u001b[0;36m_parse_example_raw\u001b[1;34m(serialized, names, params, name)\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[0mragged_split_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mragged_split_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mdense_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_shapes_as_proto\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    363\u001b[0m     (sparse_indices, sparse_values, sparse_shapes, dense_values,\n\u001b[0;32m    364\u001b[0m      ragged_values, ragged_row_splits) = outputs\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_parsing_ops.py\u001b[0m in \u001b[0;36mparse_example_v2\u001b[1;34m(serialized, names, sparse_keys, dense_keys, ragged_keys, dense_defaults, num_sparse, sparse_types, ragged_value_types, ragged_split_types, dense_shapes, name)\u001b[0m\n\u001b[0;32m    743\u001b[0m           \u001b[0mragged_value_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mragged_value_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m           \u001b[0mragged_split_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mragged_split_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdense_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m           name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m    746\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m       \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_parsing_ops.py\u001b[0m in \u001b[0;36mparse_example_v2_eager_fallback\u001b[1;34m(serialized, names, sparse_keys, dense_keys, ragged_keys, dense_defaults, num_sparse, sparse_types, ragged_value_types, ragged_split_types, dense_shapes, name, ctx)\u001b[0m\n\u001b[0;32m    836\u001b[0m                              \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mragged_value_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m                              \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mragged_split_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_inputs_flat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[0;32m    839\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m     _execute.record_gradient(\n",
      "\u001b[1;32m~\\Documents\\Envs\\tf2_py3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Feature: age_int64list (data type: int64) is required but could not be found. [Op:ParseExampleV2]"
     ]
    }
   ],
   "source": [
    "dataset_zip = tf.data.TFRecordDataset([filename_fullpath_zip], \n",
    "                                      compression_type= \"GZIP\")\n",
    "for serialized_example_tensor in dataset_zip:\n",
    "    example = tf.io.parse_single_example(\n",
    "        serialized_example_tensor,\n",
    "        expected_features)\n",
    "    books = tf.sparse.to_dense(example[\"favorite_books\"],\n",
    "                               default_value=b\"\")\n",
    "    for book in books:\n",
    "        print(book.numpy().decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_py3",
   "language": "python",
   "name": "tf2_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
